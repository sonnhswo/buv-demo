{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sonnh/miniconda3/envs/swobuv/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:139: LangChainPendingDeprecationWarning: This class is pending deprecation and may be removed in a future version. You can swap to using the `PGVector` implementation in `langchain_postgres`. Please read the guidelines in the doc-string of this class to follow prior to migrating as there are some differences between the implementations. See <https://github.com/langchain-ai/langchain-postgres> for details aboutthe new implementation.\n",
      "  warn_deprecated(\n",
      "/Users/sonnh/miniconda3/envs/swobuv/lib/python3.11/site-packages/langchain_community/vectorstores/pgvector.py:328: LangChainPendingDeprecationWarning: Please use JSONB instead of JSON for metadata. This change will allow for more efficient querying that involves filtering based on metadata.Please note that filtering operators have been changed when using JSOB metadata to be prefixed with a $ sign to avoid name collisions with columns. If you're using an existing database, you will need to create adb migration for your metadata column to be JSONB and update your queries to use the new operators. \n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_community.chat_message_histories import (\n",
    "    StreamlitChatMessageHistory,\n",
    ")\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.retrievers import MultiVectorRetriever\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "from langchain.chains import create_history_aware_retriever\n",
    "\n",
    "from langchain.storage._lc_store import create_kv_docstore\n",
    "from langchain.storage import InMemoryStore, LocalFileStore\n",
    "\n",
    "\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from typing import List\n",
    "from utils import language_detection_chain, text_embedding_3large, azure_openai\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "__import__('pysqlite3')\n",
    "sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')\n",
    "\n",
    "load_dotenv(find_dotenv(\".env\"))\n",
    "AZURE_OPENAI_API_KEY = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "AZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "As an AI assistant specializing in student support, your task is to provide concise and comprehensive answers to specific questions based on the provided context. \n",
    "The context is a list of sources. Each source includes source name and information.\n",
    "You MUST follow instruction deliminated by ###.\n",
    "\n",
    "###\n",
    "Instructions:\n",
    "\n",
    "1. Begin by reading the context carefully.\n",
    "2. Answer the question based on the information in the context.\n",
    "3. If you donâ€™t know the answer, say \"Sorry, the documents do not mention about this information. Please contact the Student Information Office via studentservice@buv.edu.vn for further support. Thank you\". Do not fabricate responses. And Do not make up references\n",
    "4. Keep your answer as succinct as possible, but ensure it includes all relevant information from the context. For examples: \n",
    "    - if students ask about a department or services, you should answer not only department name or serivec name, but also service link and department contact such as email, phone, ... if those information have in the context. \n",
    "    - if context does not have specific answer, but contain reference information such as reference link, reference contact point, support contact point and so on. Then you should show it up.\n",
    "    - if context contains advices for specific student's action, you should show it up.\n",
    "5. Always include the source name from the context for each fact you use in the response in the following format: \n",
    "```\n",
    "{{Answer here}} \n",
    "\n",
    "Sources:\n",
    "- Source name 1\n",
    "- Source name 2\n",
    "....\n",
    "- Source name n\n",
    "```\n",
    "### \n",
    "\n",
    "--- Start Context:\n",
    "{context}\n",
    "--- End Context\n",
    "\n",
    "Note that if the previous conversations contains usefull information, you can response based on provided context and those information too. \n",
    "Only answer in English.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "contextualize_q_system_prompt = (\n",
    "    \"\"\"\n",
    "As an expert in natural language processing, your task is to transform a given student's question, which may reference prior chat history, into a standalone question that can be understood without any context from the chat history.\n",
    "Do not answer the question, simply reformulate it if necessary.\n",
    "Because If you change the question a little bit, It can lead the question to have the different meaning and lead to bot answer incorrectly.\n",
    "So You Must Prioritize returning the latest question as it is, and only reformulate it if absolutely necessary.\n",
    "Sometimes if students just say somethings and can be understood without context, not change it to the question, just keep it as it is.\n",
    "\"\"\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-14 17:27:52.829 WARNING streamlit.runtime.state.session_state_proxy: Session state does not function when running a script without `streamlit run`\n",
      "/Users/sonnh/miniconda3/envs/swobuv/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 0.4. An updated version of the class exists in the langchain-chroma package and should be used instead. To use it run `pip install -U langchain-chroma` and import as `from langchain_chroma import Chroma`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "demo_ephemeral_chat_history = StreamlitChatMessageHistory(\n",
    "    key=\"su_follow_up_memory\")\n",
    "\n",
    "def trim_messages(chain_input):\n",
    "    stored_messages = demo_ephemeral_chat_history.messages\n",
    "    if len(stored_messages) <= 2:\n",
    "        return False\n",
    "\n",
    "    demo_ephemeral_chat_history.clear()\n",
    "\n",
    "    for message in stored_messages[-2:]:\n",
    "        demo_ephemeral_chat_history.add_message(message)\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "def format_docs_with_sources(docs: List[Document]) -> str:\n",
    "    formatted = []\n",
    "    for i, doc in enumerate(docs):\n",
    "        doc_str = f\"\"\"\\\n",
    "        Source Name: {doc.metadata['file_name']} - Page {doc.metadata['page']}\n",
    "        Information: {doc.page_content}\n",
    "        \"\"\"\n",
    "        formatted.append(doc_str)\n",
    "    return \"\\n\\n\".join(formatted)\n",
    "\n",
    "vectorstore_chunk_zie_400 = Chroma(\n",
    "    persist_directory=\"./processed_data/chroma_db/su_embedding_400_large_with_source\", embedding_function=text_embedding_3large\n",
    ")\n",
    "\n",
    "fs = LocalFileStore(\n",
    "    \"./processed_data/parent_document_store/su_embedding_large_with_source\")\n",
    "store = create_kv_docstore(fs)\n",
    "parent_document_retriever = MultiVectorRetriever(\n",
    "    vectorstore=vectorstore_chunk_zie_400,\n",
    "    docstore=store,\n",
    "    search_kwargs={\"k\": 2},\n",
    ")\n",
    "\n",
    "\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "parent_document_with_history_aware_retriever = create_history_aware_retriever(\n",
    "    azure_openai, parent_document_retriever, contextualize_q_prompt\n",
    ")\n",
    "\n",
    "custom_retriever_chain = parent_document_with_history_aware_retriever | format_docs_with_sources\n",
    "\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "rag_chain_with_parent_retriever_with_sources = (\n",
    "    RunnablePassthrough.assign(context=custom_retriever_chain)\n",
    "    | qa_prompt\n",
    "    | azure_openai\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chain_with_follow_up_function(message_history):\n",
    "    chain_with_message_history = RunnableWithMessageHistory(\n",
    "        rag_chain_with_parent_retriever_with_sources,\n",
    "        lambda session_id: message_history,\n",
    "        input_messages_key=\"input\",\n",
    "        history_messages_key=\"chat_history\",\n",
    "    )\n",
    "    chain_with_follow_up = (\n",
    "        RunnablePassthrough.assign(messages_trimmed=trim_messages)\n",
    "        | chain_with_message_history\n",
    "    )\n",
    "    return chain_with_follow_up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "swobuv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
